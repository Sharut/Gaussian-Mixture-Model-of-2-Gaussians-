# Gaussian-Mixture-Model-of-2-Gaussians-



# Explaining with a real life example:

Imagine that a researcher gives you a data set to analyse consisting of the measured heights of 100 adults. The problem is, the researcher failed to distinguish between males and females when taking measurements. When you plot the distribution of heights you see two clear peaks - most likely corresponding to a peak in male heights and a peak in female heights. Unfortunately, it is difficult to see from this messy data what is the average height of men and the average height of women, and it is even more difficult to say if a particular observation was a man or a woman in retrospect.
After firing your researcher, you may want to turn to Gaussian mixture models (GMMs) for a practical solution. A GMM assumes that the observed data is made up of a mixture of several Gaussian distributions. These individual distributions (referred to as mixture components) may be given different means and variances. They may also be given different mixture weights. The finaldistribution is obtained by multiplying each mixture component by its associated mixture weight before and adding them together (mixture weights must sum to one). In this example we would want two Gaussian distributions - one for men and one for women - with different means and perhaps different variances, and where the mixture weights correspond to the probability of a random individual being male or female (roughly 0.5 in each case).
Creating a mixture like the one above is very easy. Inferring the values of the unknown means, variances and mixture weights from an observed data set is slightly more tricky. Two commonly used methods are maximum-likelihood estimation and Bayesian posterior inference.
Maximum likelihood usually works via the Expectation-maximisation (EM) algorithm. In each step of the algorithm the unknown parameters are updated to their conditional maximum likelihood values, given everything else. After a few steps the values converge to a local optimum. We have no guarantee that this local optimum is globally the best solution, but by running the algorithm from many dispersed starting positions we can at least gain some confidence in our result. Dempster, Laird and Rubin (1977) "Maximum Likelihood from Incomplete Data via the EM Algorithm" is the standard citation in this area, although a quick Google search for "EM Gaussian Mixture" returns many more gentle introductions.
Bayesian inference requires us to specify priors on our unknown parameters, and then proceeds via Markov Chain Monte Carlo (MCMC). The final algorithm is similar to the EM algorithm, except that the unknown values are drawn at random from the appropriate distribution in each step, rather than being set at the maximum likelihood value. If conjugate priors are used then very efficient algorithms can be designed by Gibbs sampling, or for more general priors a Metropolis-Hastings step can be used. The expressions that are required as part of this algorithm in fact very simple - far simpler than you might think from browsing the literature. If you are interested in pursuing this angle let me know and I'll write them down in a reply as simply as I can.
The EM approach has the advantage of being very quick - the algorithm often converges within 10 or so iterations. It is most appropriate when the different clusters in the data are very obvious, so there is little variance around the maximum likelihood estimate. The Bayesian approach takes longer, as many samples are needed in order to produce an accurate reconstruction of the posterior distribution. The advantage of this approach, however, is that the uncertainty in the final values is captured completely. For example, following an MCMC one could write down the posterior probability that any given observation was male or female based on the observed height alone. As with all Bayesian procedures, the influence of the prior can be made small relative to the data whenever the sample size is reasonably large, or alternatively, strong informative priors can be used to integrate alternative sources of information when they are available.
There are some nuances to mixture models that can be tricky to overcome - for example the "label-switching problem", or the issue of estimating the number of mixture components - but the basic framework of GMMs is straightforward. The added flexibility that comes from using mixtures, compared with simple parametric forms, is worth the effort in many applications.
